{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VinOnyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\VinOnyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import re, string, unicodedata\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import wikipedia as wk\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('Corona.txt', 'r',errors='ignore')\n",
    "#data = open('HR.txt', 'r',errors='ignore')\n",
    "raw = data.read()\n",
    "raw = raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(text):\n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "    #word tokenization\n",
    "    word_token = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
    "    \n",
    "    #remove ASCII\n",
    "    new_words = []\n",
    "    for word in word_token:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8','ignore')\n",
    "        new_words.append(new_word)\n",
    "        \n",
    "    #remove tags\n",
    "    rmv = []\n",
    "    for w in new_words:\n",
    "        text = re.sub(\"&lt;/?.*?&gt;\", \"&lt;&gt;\", w)\n",
    "        rmv.append(text)\n",
    "    \n",
    "    #pos tagging and lemmatization\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemma_list=[]\n",
    "    rmv = [i for i in rmv if i]\n",
    "    for token, tag in nltk.pos_tag(rmv):\n",
    "        lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
    "        lemma_list.append(lemma)\n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "welcome_input = (\"hello\", \"hi\", \"greetings\", \"hey\", \"whats up\", \"niaje\", \"wachane\")\n",
    "welcome_response = [\"hi\", \"hey\", \"hi there\", \"poa sana\", \"antie\", \"Iam glad you are talking to me\"]\n",
    "def welcome(user_response):\n",
    "    for word in user_response.split():\n",
    "        if word.lower() in welcome_input:\n",
    "            return random.choice(welcome_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateResponse(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=Normalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    #vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    vals = linear_kernel(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0) or \"tell me about\" in user_response:\n",
    "        print(\"Checking Wikipedia\")\n",
    "        if user_response:\n",
    "            robo_response = wikipedia_data(user_response)\n",
    "            return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "    \n",
    "#wikipedia search\n",
    "def wikipedia_data(input):\n",
    "    reg_ex = re.search('tell me about (.*)', input)\n",
    "    try:\n",
    "        if reg_ex:\n",
    "            topic = reg_ex.group(1)\n",
    "            wiki = wk.summary(topic, sentences = 3)\n",
    "            return wiki\n",
    "    except Exception as e:\n",
    "            print(\"No content found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Dominic and I'm here to help you. If you want to exit, type Bye!\n",
      "\n",
      "\n",
      "cynthia\n",
      "\n",
      "\n",
      "Dominic : Checking Wikipedia\n",
      "None\n",
      "\n",
      "\n",
      "tell me about cynthia\n",
      "\n",
      "\n",
      "Dominic : Checking Wikipedia\n",
      "Cynthia is a feminine given name of Greek origin: Κυνθία, Kynthía, \"from Mount Cynthus\" on Delos island. There are various spellings for this name, and it can be abbreviated as Cindy, Cyndi, or as Cyndy.\n",
      "Cynthia was originally an epithet of the Greek goddess Artemis, who according to legend was born on Mount Cynthus.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print(\"My name is Dominic and I'm here to help you. If you want to exit, type Bye!\")\n",
    "while(flag == True):\n",
    "    print('\\n')\n",
    "    user_response = input()\n",
    "    print('\\n')\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response not in ['bye', 'shutdown', 'exit', 'quit']):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag = False\n",
    "            print('Dominic : Welcome!!!')\n",
    "        else:\n",
    "            if(welcome(user_response) != None):\n",
    "                print(\"Dominic : \" + welcome(user_response))\n",
    "            else:\n",
    "                print(\"Dominic : \", end = \"\")\n",
    "                print(generateResponse(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"Dominic : Bye!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
